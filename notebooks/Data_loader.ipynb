{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from models.vit import VisionTransformer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Data Transforms:\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GuidoDeFilippo/Columbia/Data_Mining/transformer_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = VisionTransformer(\n",
    "    img_size=32,\n",
    "    patch_size=4,\n",
    "    in_channels=3,\n",
    "    num_classes=10,\n",
    "    embed_dim=256,\n",
    "    num_heads=8,\n",
    "    hidden_dim=512,\n",
    "    num_layers=6,\n",
    "    dropout=0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1/1 [06:22<00:00, 382.70s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    model.train()\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation and logging code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the transform (you may already have this in your code)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Add any other transforms you are using\n",
    "])\n",
    "\n",
    "# Access the class labels\n",
    "class_labels = train_dataset.classes  # This is a list of class names\n",
    "class_to_idx = train_dataset.class_to_idx  # This is a dictionary mapping class names to indices\n",
    "\n",
    "print(\"Class Labels:\")\n",
    "for idx, label in enumerate(class_labels):\n",
    "    print(f\"{idx}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize if you have normalized the images\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images[:4]))\n",
    "\n",
    "# Print labels\n",
    "print('Labels:')\n",
    "for label in labels[:4]:\n",
    "    print(f\"{label.item()}: {class_labels[label]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "patch_embed\n",
      "patch_embed.proj\n",
      "pos_embed\n",
      "dropout\n",
      "encoder\n",
      "encoder.layers\n",
      "encoder.layers.0\n",
      "encoder.layers.0.self_attn\n",
      "encoder.layers.0.self_attn.query_proj\n",
      "encoder.layers.0.self_attn.key_proj\n",
      "encoder.layers.0.self_attn.value_proj\n",
      "encoder.layers.0.self_attn.out_proj\n",
      "encoder.layers.0.linear1\n",
      "encoder.layers.0.dropout\n",
      "encoder.layers.0.linear2\n",
      "encoder.layers.0.norm1\n",
      "encoder.layers.0.norm2\n",
      "encoder.layers.0.dropout1\n",
      "encoder.layers.0.dropout2\n",
      "encoder.layers.0.activation\n",
      "encoder.layers.1\n",
      "encoder.layers.1.self_attn\n",
      "encoder.layers.1.self_attn.query_proj\n",
      "encoder.layers.1.self_attn.key_proj\n",
      "encoder.layers.1.self_attn.value_proj\n",
      "encoder.layers.1.self_attn.out_proj\n",
      "encoder.layers.1.linear1\n",
      "encoder.layers.1.dropout\n",
      "encoder.layers.1.linear2\n",
      "encoder.layers.1.norm1\n",
      "encoder.layers.1.norm2\n",
      "encoder.layers.1.dropout1\n",
      "encoder.layers.1.dropout2\n",
      "encoder.layers.1.activation\n",
      "encoder.layers.2\n",
      "encoder.layers.2.self_attn\n",
      "encoder.layers.2.self_attn.query_proj\n",
      "encoder.layers.2.self_attn.key_proj\n",
      "encoder.layers.2.self_attn.value_proj\n",
      "encoder.layers.2.self_attn.out_proj\n",
      "encoder.layers.2.linear1\n",
      "encoder.layers.2.dropout\n",
      "encoder.layers.2.linear2\n",
      "encoder.layers.2.norm1\n",
      "encoder.layers.2.norm2\n",
      "encoder.layers.2.dropout1\n",
      "encoder.layers.2.dropout2\n",
      "encoder.layers.2.activation\n",
      "encoder.layers.3\n",
      "encoder.layers.3.self_attn\n",
      "encoder.layers.3.self_attn.query_proj\n",
      "encoder.layers.3.self_attn.key_proj\n",
      "encoder.layers.3.self_attn.value_proj\n",
      "encoder.layers.3.self_attn.out_proj\n",
      "encoder.layers.3.linear1\n",
      "encoder.layers.3.dropout\n",
      "encoder.layers.3.linear2\n",
      "encoder.layers.3.norm1\n",
      "encoder.layers.3.norm2\n",
      "encoder.layers.3.dropout1\n",
      "encoder.layers.3.dropout2\n",
      "encoder.layers.3.activation\n",
      "mlp_head\n",
      "mlp_head.0\n",
      "mlp_head.1\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'vit_model' is your Vision Transformer model instance\n",
    "for name, module in relu_model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchcam.methods import GradCAM\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# Initialize GradCAM with the desired layer\n",
    "cam_extractor = GradCAM(relu_model, target_layer='encoder.layers.3')\n",
    "\n",
    "# Prepare your image\n",
    "# Assuming 'image' is a PIL Image or tensor of shape [C, H, W]\n",
    "image_tensor = transform_test(image).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "# Forward pass\n",
    "outputs = relu_model(image_tensor)\n",
    "\n",
    "# Get the predicted class index\n",
    "pred_class = outputs.argmax(dim=1).item()\n",
    "\n",
    "# Retrieve the CAM by passing the class index and model outputs\n",
    "activation_map = cam_extractor(class_idx=pred_class, scores=outputs)\n",
    "\n",
    "# The activation_map is a list of activation maps for each target layer specified.\n",
    "# Since we have only one target layer, we can get the first element.\n",
    "activation_map = activation_map[0]\n",
    "\n",
    "# Convert image tensor back to PIL Image for visualization\n",
    "input_image = to_pil_image(image_tensor.squeeze(0).cpu())\n",
    "\n",
    "# Resize the activation map to match the input image size\n",
    "import torch.nn.functional as F\n",
    "activation_map_resized = F.interpolate(\n",
    "    activation_map.unsqueeze(0).unsqueeze(0), size=(input_image.size[1], input_image.size[0]), mode='bilinear', align_corners=False\n",
    ").squeeze().cpu().numpy()\n",
    "\n",
    "# Visualize the CAM overlayed on the input image\n",
    "plt.imshow(input_image)\n",
    "plt.imshow(activation_map_resized, cmap='jet', alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
